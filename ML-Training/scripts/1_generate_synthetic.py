#!/usr/bin/env python3
"""
Generate synthetic training data using Google Gemini Flash

Strategy:
1. Paraphrasing: Generate 5 variations per sample
2. Style Transfer: AI â†” HUMAN style conversion
3. Augmentation: Length, tone, word choice variations
4. Zero-shot Generation: Create new blog posts

Target: 10,000 synthetic samples from 297 real samples
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
import google.generativeai as genai
from dotenv import load_dotenv


# Load environment variables from ML-Training/.env
# Get the directory where this script is located
SCRIPT_DIR = Path(__file__).parent  # ML-Training/scripts/
PROJECT_ROOT = SCRIPT_DIR.parent  # ML-Training/
ENV_PATH = PROJECT_ROOT / ".env"

# Load .env file with override=True to ignore existing env vars
load_dotenv(dotenv_path=ENV_PATH, override=True)

# Debug: Print if .env was loaded
if ENV_PATH.exists():
    print(f"âœ“ Loaded .env from {ENV_PATH}")
else:
    print(f"âš  Warning: .env file not found at {ENV_PATH}")


@dataclass
class SyntheticConfig:
    """Configuration for synthetic data generation"""
    real_data_path: str
    output_dir: str
    target_size: int = 10000
    gemini_model: str = "gemini-flash-latest"
    variations_per_sample: int = 5
    temperature: float = 0.9
    max_retries: int = 3
    batch_size: int = 10
    max_workers: int = 40  # Parallel API calls


class GeminiSyntheticGenerator:
    """Generate synthetic data using Gemini Flash"""

    def __init__(self, config: SyntheticConfig):
        self.config = config

        # Initialize Gemini
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError(
                "GEMINI_API_KEY not found in environment.\n"
                f"Please set it in {ENV_PATH} file or export it.\n"
                f"Example: GEMINI_API_KEY=your_api_key_here"
            )

        # Debug: Print API key info (first/last 4 chars only)
        if len(api_key) > 8:
            masked_key = f"{api_key[:4]}...{api_key[-4:]}"
            print(f"âœ“ Found API key: {masked_key}")
        else:
            print(f"âš  Warning: API key seems too short ({len(api_key)} chars)")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(config.gemini_model)

        print(f"âœ“ Initialized Gemini model: {config.gemini_model}")

    def load_real_data(self) -> List[Dict]:
        """Load real training data"""
        with open(self.config.real_data_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        print(f"âœ“ Loaded {len(data)} real samples")
        return data

    def generate_paraphrases(
        self,
        text: str,
        label: str,
        num_variations: int = 5,
    ) -> List[Dict]:
        """
        Generate paraphrased variations of text

        Args:
            text: Original text
            label: AI or HUMAN
            num_variations: Number of variations to generate

        Returns:
            List of synthetic samples
        """
        # Style-specific guidelines based on labeling criteria
        if label == "AI":
            style_guide = """**AI ìŠ¤íƒ€ì¼ íŠ¹ì„± ìœ ì§€**:
1. ì„œì‚¬ì˜ ì§„ì •ì„±: ì „í˜•ì ì´ê³  ê·¸ëŸ´ë“¯í•œ ì´ì•¼ê¸° êµ¬ì¡°
2. ë””í…Œì¼: ì¼ë°˜ì ì´ê³  ìƒíˆ¬ì ì¸ í‘œí˜„ ("ë§›ìžˆì—ˆë‹¤", "ì¢‹ì•˜ë‹¤")
3. ë¹„íŒì˜ ê· í˜•: í˜•ì‹ì ì´ê³  ê°€ë²¼ìš´ ë‹¨ì  ì–¸ê¸‰
4. ì–¸ì–´: "í™˜ìƒì ì¸", "ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ëŠ”" ë“± ë¦¬ë·° ìƒíˆ¬ì–´ ì‚¬ìš©
5. ì–´ì¡°: ì¼ê´€ë˜ê²Œ ì •ì¤‘í•˜ê³  ê¸ì •ì , ê°ì • ë³€í™” ìµœì†Œí™”
6. êµ¬ì¡°: ê· ì¼í•œ ë¬¸ë‹¨ ê¸¸ì´, í‚¤ì›Œë“œ ë°˜ë³µ, ì •í˜•í™”ëœ íë¦„
7. ëª©ì : ì •ë³´ ì „ë‹¬ ì¤‘ì‹¬, ìƒì—…ì  ì˜ë„ ë‚´í¬"""
        else:
            style_guide = """**HUMAN ìŠ¤íƒ€ì¼ íŠ¹ì„± ìœ ì§€**:
1. ì„œì‚¬ì˜ ì§„ì •ì„±: ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê³  ì‚¬ì†Œí•œ ë””í…Œì¼ì´ ì‚´ì•„ìžˆëŠ” ì‹¤ì œ ê²½í—˜
2. ë””í…Œì¼: ì˜¤ê°ì„ ìžê·¹í•˜ëŠ” êµ¬ì²´ì ì´ê³  ê°ê°ì ì¸ ë¬˜ì‚¬
3. ë¹„íŒì˜ ê· í˜•: ì‹¤ì œ ë¶ˆíŽ¸í•¨ì—ì„œ ë‚˜ì˜¨ ì§„ì†”í•œ í”¼ë“œë°±
4. ì–¸ì–´: ê°œì¸ë§Œì˜ ë…ì°½ì ì´ê³  ì‹ ì„ í•œ í‘œí˜„
5. ì–´ì¡°: êµ¬ì–´ì²´ ì‚¬ìš© ("ê·¸ì¹˜ë§Œ", "ê·¼ë°", "ì§„ì§œ"), ê°ì • ë³€í™” ìžì—°ìŠ¤ëŸ¬ì›€
6. êµ¬ì¡°: ë¶ˆê·œì¹™ì  ë¬¸ë‹¨, ìžì—°ìŠ¤ëŸ¬ìš´ íë¦„, ê°œì¸ ì‚¬ê³ ì˜ ì „ê°œ
7. ëª©ì : ìˆœìˆ˜í•œ ê²½í—˜ ê³µìœ , ê°œì¸ì  ê°ì • í‘œí˜„"""

        prompt = f"""ë‹¹ì‹ ì€ 'ë””ì§€í„¸ í…ìŠ¤íŠ¸ ë²•ì˜í•™ìž'ë¡œì„œ ë¸”ë¡œê·¸ ê¸€ì˜ AI/HUMAN íŠ¹ì„±ì„ ì •í™•ížˆ ì´í•´í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

ì›ë³¸ ê¸€:
{text}

ì›ë³¸ ë¼ë²¨: {label}

**ìž„ë¬´**: ìœ„ ì›ë³¸ ê¸€ì˜ ì˜ë¯¸ì™€ {label} ìŠ¤íƒ€ì¼ íŠ¹ì„±ì„ ì •í™•ížˆ ìœ ì§€í•˜ë©´ì„œ {num_variations}ê°œì˜ ìžì—°ìŠ¤ëŸ¬ìš´ ë³€í˜•ì„ ìƒì„±í•˜ì„¸ìš”.

{style_guide}

**ê·œì¹™**:
1. ì›ë³¸ì˜ í•µì‹¬ ì˜ë¯¸ì™€ {label} ìŠ¤íƒ€ì¼ íŠ¹ì„± ì™„ë²½ížˆ ìœ ì§€
2. ë‹¨ì–´ ì„ íƒ, ë¬¸ìž¥ êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•˜ê²Œ ë³€ê²½
3. ê¸¸ì´ëŠ” ì›ë³¸ì˜ 80-120% ë²”ìœ„
4. ìœ„ ìŠ¤íƒ€ì¼ ê°€ì´ë“œì˜ ëª¨ë“  íŠ¹ì„± ë°˜ì˜
5. ìžì—°ìŠ¤ëŸ½ê³  ì§„ì§œì²˜ëŸ¼ ëŠê»´ì§€ëŠ” í•œêµ­ì–´ ìž‘ì„±

**ì¶œë ¥ í˜•ì‹** (JSON):
{{
  "variations": [
    "ë³€í˜• 1 í…ìŠ¤íŠ¸",
    "ë³€í˜• 2 í…ìŠ¤íŠ¸",
    ...
  ]
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."""

        try:
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": self.config.temperature,
                    "max_output_tokens": 8192,
                }
            )

            # Parse JSON response
            response_text = response.text.strip()

            # Remove markdown code blocks more robustly
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            elif response_text.startswith("```"):
                response_text = response_text[3:]

            if response_text.endswith("```"):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            # Validate response is not empty
            if not response_text:
                print(f"âš  Empty response from API")
                return []

            # Try to parse JSON with better error handling
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError as json_err:
                print(f"âš  JSON decode error: {json_err}")
                print(f"âš  Response preview (first 200 chars): {response_text[:200]}")
                return []

            variations = result.get("variations", [])

            if not variations:
                print(f"âš  No variations in response")
                return []

            # Create synthetic samples
            samples = []
            for var_text in variations[:num_variations]:
                if var_text and var_text.strip():
                    samples.append({
                        "text": var_text,
                        "label": label,
                        "source": "synthetic_paraphrase",
                        "metadata": {
                            "original_text": text[:100] + "...",
                            "generation_method": "paraphrase",
                        }
                    })

            return samples

        except Exception as e:
            print(f"âš  Error generating paraphrases: {type(e).__name__}: {e}")
            return []

    def generate_style_transfer(
        self,
        text: str,
        source_label: str,
        target_label: str,
    ) -> Optional[Dict]:
        """
        Transfer style from AI to HUMAN or vice versa

        Args:
            text: Original text
            source_label: Original label (AI or HUMAN)
            target_label: Target label (AI or HUMAN)

        Returns:
            Synthetic sample with transferred style
        """
        if source_label == "AI":
            style_instruction = """**AI â†’ HUMAN ë³€í™˜ (7ê°€ì§€ í•µì‹¬ ê¸°ì¤€)**:

1. **ì„œì‚¬ì˜ ì§„ì •ì„±**: ì „í˜•ì  ì´ì•¼ê¸° â†’ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì‹¤ì œ ê²½í—˜
   - Before: "ë§¤ìž¥ì— ë°©ë¬¸í–ˆê³  ìŒì‹ì„ ì£¼ë¬¸í–ˆìŠµë‹ˆë‹¤"
   - After: "ìš°ì—°ížˆ ì§€ë‚˜ê°€ë‹¤ê°€ ì‚¬ëžŒ ë§Žê¸¸ëž˜ ë“¤ì–´ê°”ëŠ”ë°"

2. **ë””í…Œì¼ì˜ êµ¬ì²´ì„±**: ì¼ë°˜ì  ë¬˜ì‚¬ â†’ ì˜¤ê° ìžê·¹ êµ¬ì²´ì  ë¬˜ì‚¬
   - Before: "ìŒì‹ì´ ë§›ìžˆì—ˆìŠµë‹ˆë‹¤"
   - After: "ê°“ êµ¬ìš´ ë¹µì—ì„œ ë²„í„° í–¥ì´ í™• ì˜¬ë¼ì˜¤ë”ë¼ê³ ìš”"

3. **ë¹„íŒì˜ ê· í˜•**: í˜•ì‹ì  ë‹¨ì  â†’ ì§„ì†”í•œ ë¶ˆë§Œ
   - Before: "ë‹¤ë§Œ ê°€ê²©ì´ ì¡°ê¸ˆ ë¹„ì‹¼ íŽ¸ìž…ë‹ˆë‹¤"
   - After: "ì†”ì§ížˆ ì´ ê°€ê²©ì— ì´ ì–‘ì´ë©´ ì¢€ ì•„ì‰¬ì› ì–´ìš”"

4. **ì–¸ì–´ì˜ ë…ì°½ì„±**: ë¦¬ë·° ìƒíˆ¬ì–´ â†’ ê°œì¸ì  í‘œí˜„
   - Before: "í™˜ìƒì ì¸ ê²½í—˜", "ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ëŠ”"
   - After: "ì™„ì „ ëŒ€ë°•", "ì—„ì²­ ì¢‹ë”ë¼ê³ "

5. **ì–´ì¡°ì™€ ê°ì •**: ì¼ê´€ëœ ì •ì¤‘í•¨ â†’ ìžì—°ìŠ¤ëŸ¬ìš´ ê°ì • ë³€í™”
   - êµ¬ì–´ì²´ ì¶”ê°€: "ê·¸ì¹˜ë§Œ", "ê·¼ë°", "ì§„ì§œ", "ã…‹ã…‹"
   - ì¢…ê²°ì–´ë¯¸: "~ìž…ë‹ˆë‹¤" â†’ "~ë„¤ìš”", "~ìš”", "~ë”ë¼"

6. **êµ¬ì¡°ì˜ ìžì—°ìŠ¤ëŸ¬ì›€**: ê· ì¼í•œ ë¬¸ë‹¨ â†’ ë¶ˆê·œì¹™ì  íë¦„
   - í‚¤ì›Œë“œ ë°˜ë³µ ì œê±°
   - ë¬¸ë‹¨ ê¸¸ì´ ë¶ˆê·œì¹™í•˜ê²Œ
   - ê°œì¸ ì‚¬ê³  íë¦„ ë°˜ì˜

7. **ê¶ê·¹ì  ëª©ì **: ì •ë³´ ì „ë‹¬ â†’ ê²½í—˜ ê³µìœ 
   - ìƒì—…ì  ëŠë‚Œ ì œê±°
   - ê°œì¸ ê²½í—˜ê³¼ ê°ì • ì¤‘ì‹¬ìœ¼ë¡œ"""
        else:
            style_instruction = """**HUMAN â†’ AI ë³€í™˜ (7ê°€ì§€ í•µì‹¬ ê¸°ì¤€)**:

1. **ì„œì‚¬ì˜ ì§„ì •ì„±**: ì‹¤ì œ ê²½í—˜ â†’ ì „í˜•ì ì´ê³  ê·¸ëŸ´ë“¯í•œ êµ¬ì¡°
   - Before: "ìš°ì—°ížˆ ì§€ë‚˜ê°€ë‹¤ê°€ ì‚¬ëžŒ ë§Žê¸¸ëž˜ ë“¤ì–´ê°”ëŠ”ë°"
   - After: "ë§¤ìž¥ì„ ë°©ë¬¸í•˜ì—¬ ìŒì‹ì„ ì£¼ë¬¸í–ˆìŠµë‹ˆë‹¤"

2. **ë””í…Œì¼ì˜ êµ¬ì²´ì„±**: ê°ê°ì  ë¬˜ì‚¬ â†’ ì¼ë°˜ì  í‘œí˜„
   - Before: "ê°“ êµ¬ìš´ ë¹µì—ì„œ ë²„í„° í–¥ì´ í™• ì˜¬ë¼ì˜¤ë”ë¼ê³ ìš”"
   - After: "ìŒì‹ì´ ë§›ìžˆì—ˆìŠµë‹ˆë‹¤"

3. **ë¹„íŒì˜ ê· í˜•**: ì§„ì†”í•œ ë¶ˆë§Œ â†’ í˜•ì‹ì  ë‹¨ì  ì–¸ê¸‰
   - Before: "ì†”ì§ížˆ ì´ ê°€ê²©ì— ì´ ì–‘ì´ë©´ ì¢€ ì•„ì‰¬ì› ì–´ìš”"
   - After: "ë‹¤ë§Œ ê°€ê²©ì´ ì¡°ê¸ˆ ë¹„ì‹¼ íŽ¸ìž…ë‹ˆë‹¤"

4. **ì–¸ì–´ì˜ ë…ì°½ì„±**: ê°œì¸ì  í‘œí˜„ â†’ ë¦¬ë·° ìƒíˆ¬ì–´
   - Before: "ì™„ì „ ëŒ€ë°•", "ì—„ì²­ ì¢‹ë”ë¼ê³ "
   - After: "í™˜ìƒì ì¸ ê²½í—˜", "ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ëŠ”"

5. **ì–´ì¡°ì™€ ê°ì •**: ìžì—°ìŠ¤ëŸ¬ìš´ ë³€í™” â†’ ì¼ê´€ëœ ì •ì¤‘í•¨
   - êµ¬ì–´ì²´ ì œê±°: "ê·¸ì¹˜ë§Œ", "ê·¼ë°", "ì§„ì§œ" â†’ ë¬¸ì–´ì²´
   - ì¢…ê²°ì–´ë¯¸: "~ë„¤ìš”", "~ìš”" â†’ "~ìž…ë‹ˆë‹¤", "~ìŠµë‹ˆë‹¤"

6. **êµ¬ì¡°ì˜ ì¸ìœ„ì„±**: ìžì—°ìŠ¤ëŸ¬ìš´ íë¦„ â†’ ê· ì¼í•˜ê³  ì •í˜•í™”
   - í‚¤ì›Œë“œ ì ì ˆížˆ ë°˜ë³µ
   - ë¬¸ë‹¨ ê¸¸ì´ ê· ì¼í•˜ê²Œ
   - ë…¼ë¦¬ì  êµ¬ì¡° ê°•í™”

7. **ê¶ê·¹ì  ëª©ì **: ê²½í—˜ ê³µìœ  â†’ ì •ë³´ ì „ë‹¬
   - ê°ê´€ì  ì •ë³´ ì¤‘ì‹¬
   - ìƒì—…ì  ì˜ë„ ë‚´í¬ ê°€ëŠ¥"""

        prompt = f"""ë‹¹ì‹ ì€ 'ë””ì§€í„¸ í…ìŠ¤íŠ¸ ë²•ì˜í•™ìž'ë¡œì„œ AIì™€ HUMAN ê¸€ì˜ ì°¨ì´ë¥¼ ì •í™•ížˆ ì•Œê³  ìžˆìŠµë‹ˆë‹¤.

ì›ë³¸ ê¸€ ({source_label} ìŠ¤íƒ€ì¼):
{text}

**ìž„ë¬´**: ìœ„ ê¸€ì„ {target_label} ìŠ¤íƒ€ì¼ë¡œ ì™„ë²½ížˆ ë³€í™˜í•˜ì„¸ìš”.

{style_instruction}

**ì¤‘ìš”**: ìœ„ 7ê°€ì§€ ê¸°ì¤€ì„ ëª¨ë‘ ì ìš©í•˜ì—¬, {target_label} ê¸€ì˜ íŠ¹ì„±ì„ ì™„ë²½ížˆ ìž¬í˜„í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹** (JSON):
{{
  "converted_text": "ë³€í™˜ëœ í…ìŠ¤íŠ¸"
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": self.config.temperature,
                    "max_output_tokens": 8192,
                }
            )

            response_text = response.text.strip()

            # Remove markdown code blocks more robustly
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            elif response_text.startswith("```"):
                response_text = response_text[3:]

            if response_text.endswith("```"):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            # Validate response is not empty
            if not response_text:
                print(f"âš  Empty response from API")
                return None

            # Try to parse JSON with better error handling
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError as json_err:
                # Log the problematic response for debugging
                print(f"âš  JSON decode error: {json_err}")
                print(f"âš  Response preview (first 200 chars): {response_text[:200]}")
                return None

            converted_text = result.get("converted_text", "")

            if converted_text and converted_text.strip():
                return {
                    "text": converted_text,
                    "label": target_label,
                    "source": "synthetic_style_transfer",
                    "metadata": {
                        "original_label": source_label,
                        "target_label": target_label,
                        "generation_method": "style_transfer",
                    }
                }
            else:
                print(f"âš  Empty converted_text in response")
                return None

        except Exception as e:
            print(f"âš  Error in style transfer: {type(e).__name__}: {e}")

        return None

    def generate_zero_shot(
        self,
        label: str,
        num_samples: int = 1,
    ) -> List[Dict]:
        """
        Generate completely new blog posts

        Args:
            label: AI or HUMAN
            num_samples: Number of samples to generate

        Returns:
            List of new synthetic samples
        """
        if label == "AI":
            style_guide = """**AI ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (7ê°€ì§€ íŠ¹ì§•)**:

1. **ì„œì‚¬ì˜ ì§„ì •ì„±**: ì „í˜•ì ì´ê³  ê·¸ëŸ´ë“¯í•œ ì´ì•¼ê¸°
   - "ë§¤ìž¥ì„ ë°©ë¬¸í•˜ì—¬", "ìƒí’ˆì„ êµ¬ë§¤í–ˆê³ ", "ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í–ˆìŠµë‹ˆë‹¤"
   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì „ê°œ, ë§Œë“¤ì–´ì§„ ëŠë‚Œ

2. **ë””í…Œì¼ì˜ êµ¬ì²´ì„±**: ì¼ë°˜ì ì´ê³  ìƒíˆ¬ì ì¸ í‘œí˜„
   - "ë§›ìžˆì—ˆìŠµë‹ˆë‹¤", "ì¢‹ì•˜ìŠµë‹ˆë‹¤", "ì¶”ì²œë“œë¦½ë‹ˆë‹¤"
   - êµ¬ì²´ì  ê°ê° ë¬˜ì‚¬ ìµœì†Œí™”

3. **ë¹„íŒì˜ ê· í˜•**: í˜•ì‹ì ì´ê³  ê°€ë²¼ìš´ ë‹¨ì  ì–¸ê¸‰
   - "ë‹¤ë§Œ ~í•œ ì ì´ ì•„ì‰½ìŠµë‹ˆë‹¤", "ì¡°ê¸ˆ ~í•œ íŽ¸ìž…ë‹ˆë‹¤"

4. **ì–¸ì–´ì˜ ë…ì°½ì„±**: ë¦¬ë·° ìƒíˆ¬ì–´ ì‚¬ìš©
   - "í™˜ìƒì ì¸", "ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ëŠ”", "ì™„ë²½í•œ", "ìµœê³ ì˜"
   - ì˜¨ë¼ì¸ ë¦¬ë·° ì „í˜•ì  í‘œí˜„

5. **ì–´ì¡°ì™€ ê°ì •**: ì¼ê´€ë˜ê²Œ ì •ì¤‘í•˜ê³  ê¸ì •ì 
   - "~ìž…ë‹ˆë‹¤", "~ìŠµë‹ˆë‹¤", "~ì˜€ìŠµë‹ˆë‹¤"
   - ê°ì • ë³€í™” ìµœì†Œ, ê· ì¼í•œ í†¤

6. **êµ¬ì¡°ì˜ ì¸ìœ„ì„±**: ê· ì¼í•˜ê³  ì •í˜•í™”ëœ êµ¬ì¡°
   - í‚¤ì›Œë“œ ë°˜ë³µ (SEO ì˜ë„)
   - ê· ì¼í•œ ë¬¸ë‹¨ ê¸¸ì´
   - ì„œë¡ -ë³¸ë¡ -ê²°ë¡  ëª…í™•

7. **ê¶ê·¹ì  ëª©ì **: ì •ë³´ ì „ë‹¬ ë° ìƒì—…ì  ì˜ë„
   - ê°ê´€ì  ì •ë³´ ë‚˜ì—´
   - ë§í¬ ìœ ë„, ê´‘ê³ ì„± ëŠë‚Œ"""
        else:
            style_guide = """**HUMAN ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (7ê°€ì§€ íŠ¹ì§•)**:

1. **ì„œì‚¬ì˜ ì§„ì •ì„±**: ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì‹¤ì œ ê²½í—˜
   - "ìš°ì—°ížˆ ì§€ë‚˜ê°€ë‹¤ê°€", "ì¹œêµ¬ ì¶”ì²œìœ¼ë¡œ", "ì›ëž˜ëŠ” ì•ˆ ê°ˆ ë»” í–ˆëŠ”ë°"
   - ì‚¬ì†Œí•œ ë””í…Œì¼, ì§„ì§œ ê²½í—˜ ëŠë‚Œ

2. **ë””í…Œì¼ì˜ êµ¬ì²´ì„±**: ì˜¤ê° ìžê·¹ ê°ê°ì  ë¬˜ì‚¬
   - "ê³ ì†Œí•œ ë²„í„° í–¥ì´ í™• ì˜¬ë¼ì˜¤ë”ë¼ê³ ìš”"
   - "ê²‰ì€ ë°”ì‚­í•œë° ì†ì€ ì´‰ì´‰í–ˆì–´ìš”"
   - êµ¬ì²´ì ì´ê³  ìƒìƒí•œ ê°ê°

3. **ë¹„íŒì˜ ê· í˜•**: ì§„ì†”í•œ ë¶ˆë§Œê³¼ í”¼ë“œë°±
   - "ì†”ì§ížˆ ë³„ë¡œì˜€ì–´ìš”", "ì´ ê°€ê²©ì— ì´ê±´ ì¢€..."
   - "ì²˜ìŒì—” ì¢‹ì•˜ëŠ”ë° ë‚˜ì¤‘ì—” ì‹¤ë§"

4. **ì–¸ì–´ì˜ ë…ì°½ì„±**: ê°œì¸ë§Œì˜ í‘œí˜„
   - "ì™„ì „ ëŒ€ë°•", "ì§„ì§œ ê°œì©”ì—ˆì–´", "ì—„ì²­ ì¢‹ë”ë¼ê³ "
   - ë…íŠ¹í•˜ê³  ì‹ ì„ í•œ í‘œí˜„

5. **ì–´ì¡°ì™€ ê°ì •**: ìžì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ì™€ ê°ì • ë³€í™”
   - "ê·¸ì¹˜ë§Œ", "ê·¼ë°", "ì§„ì§œ", "ã…‹ã…‹", "ã… ã… "
   - "~ë„¤ìš”", "~ìš”", "~ë”ë¼", "~ë˜ë°"
   - ì„¤ë ˜, ì‹¤ë§, ë§Œì¡± ë“± ê°ì • ë³€í™”

6. **êµ¬ì¡°ì˜ ìžì—°ìŠ¤ëŸ¬ì›€**: ë¶ˆê·œì¹™ì ì´ê³  ìžìœ ë¡œìš´ íë¦„
   - ë¬¸ë‹¨ ê¸¸ì´ ë¶ˆê·œì¹™
   - í‚¤ì›Œë“œ ë°˜ë³µ ì—†ìŒ
   - ìƒê°ì˜ íë¦„ëŒ€ë¡œ ì „ê°œ

7. **ê¶ê·¹ì  ëª©ì **: ìˆœìˆ˜í•œ ê²½í—˜ ê³µìœ 
   - ê°œì¸ ê²½í—˜ê³¼ ê°ì • ì¤‘ì‹¬
   - ìƒì—…ì  ì˜ë„ ì—†ìŒ
   - ì§„ì†”í•œ ê³µìœ """

        topics = [
            "ë§›ì§‘ í›„ê¸°", "ì—¬í–‰ ê²½í—˜", "ì œí’ˆ ë¦¬ë·°", "ì¼ìƒ ì´ì•¼ê¸°",
            "ì·¨ë¯¸ í™œë™", "ê±´ê°• ì •ë³´", "ìš”ë¦¬ ë ˆì‹œí”¼", "ì˜í™”/ë“œë¼ë§ˆ ê°ìƒí‰",
            "ë…ì„œ í›„ê¸°", "ì‡¼í•‘ ê²½í—˜", "ìš´ë™ ì¼ì§€", "IT ì œí’ˆ ì‚¬ìš©ê¸°",
        ]

        topic = random.choice(topics)

        prompt = f"""ë‹¹ì‹ ì€ 'ë””ì§€í„¸ í…ìŠ¤íŠ¸ ë²•ì˜í•™ìž'ë¡œì„œ AIì™€ HUMAN ê¸€ì˜ ì°¨ì´ë¥¼ ì •í™•ížˆ ì´í•´í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

**ì£¼ì œ**: {topic}
**ìŠ¤íƒ€ì¼**: {label}

{style_guide}

**ìž„ë¬´**: ìœ„ ì£¼ì œë¡œ 100-300ìž ê¸¸ì´ì˜ ë¸”ë¡œê·¸ ìŠ¤ë‹ˆíŽ« {num_samples}ê°œë¥¼ {label} ìŠ¤íƒ€ì¼ë¡œ ì™„ë²½ížˆ ìƒì„±í•˜ì„¸ìš”.

**ì¤‘ìš”**: ìœ„ 7ê°€ì§€ íŠ¹ì§•ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬, ì§„ì§œ {label} ê¸€ì²˜ëŸ¼ ëŠê»´ì§€ë„ë¡ ìž‘ì„±í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹** (JSON):
{{
  "posts": [
    "ë¸”ë¡œê·¸ ê¸€ 1",
    "ë¸”ë¡œê·¸ ê¸€ 2",
    ...
  ]
}}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": self.config.temperature,
                    "max_output_tokens": 8192,
                }
            )

            response_text = response.text.strip()

            # Remove markdown code blocks more robustly
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            elif response_text.startswith("```"):
                response_text = response_text[3:]

            if response_text.endswith("```"):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            # Validate response is not empty
            if not response_text:
                print(f"âš  Empty response from API")
                return []

            # Try to parse JSON with better error handling
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError as json_err:
                print(f"âš  JSON decode error: {json_err}")
                print(f"âš  Response preview (first 200 chars): {response_text[:200]}")
                return []

            posts = result.get("posts", [])

            if not posts:
                print(f"âš  No posts in response")
                return []

            samples = []
            for post_text in posts[:num_samples]:
                if post_text and post_text.strip():
                    samples.append({
                        "text": post_text,
                        "label": label,
                        "source": "synthetic_zero_shot",
                        "metadata": {
                            "topic": topic,
                            "generation_method": "zero_shot",
                        }
                    })

            return samples

        except Exception as e:
            print(f"âš  Error generating zero-shot samples: {type(e).__name__}: {e}")
            return []

    def _process_paraphrase_sample(self, sample: Dict) -> List[Dict]:
        """Process a single sample for paraphrasing (for parallel execution)"""
        # Combine title and snippet_text
        title = sample.get("title", "")
        snippet_text = sample.get("snippet_text", "")
        text = f"{title}\n\n{snippet_text}" if title and snippet_text else (title or snippet_text)
        label = sample.get("label", "HUMAN")

        if not text.strip():
            return []

        variations = self.generate_paraphrases(
            text,
            label,
            self.config.variations_per_sample
        )
        return variations

    def _process_style_transfer_sample(self, sample: Dict) -> Optional[Dict]:
        """Process a single sample for style transfer (for parallel execution)"""
        # Combine title and snippet_text
        title = sample.get("title", "")
        snippet_text = sample.get("snippet_text", "")
        text = f"{title}\n\n{snippet_text}" if title and snippet_text else (title or snippet_text)
        label = sample.get("label", "HUMAN")
        target_label = "HUMAN" if label == "AI" else "AI"

        if not text.strip():
            return None

        return self.generate_style_transfer(text, label, target_label)

    def _save_checkpoint(self, data: List[Dict], filename: str):
        """Save checkpoint data to prevent data loss"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        checkpoint_path = output_dir / filename
        with open(checkpoint_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"  ðŸ’¾ Checkpoint saved: {checkpoint_path}")

    def generate_all_synthetic_data(self) -> List[Dict]:
        """
        Generate all synthetic data using multiple strategies with parallel processing

        Returns:
            List of all synthetic samples
        """
        # Load real data
        real_data = self.load_real_data()
        all_synthetic = []

        # Strategy 1: Paraphrasing (main strategy) - PARALLEL
        print("\nðŸ“ Strategy 1: Paraphrasing (Parallel)...")
        print(f"  Generating {self.config.variations_per_sample} variations per sample")
        print(f"  Using {self.config.max_workers} concurrent workers")

        paraphrase_results = []
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # Submit all tasks
            futures = {
                executor.submit(self._process_paraphrase_sample, sample): sample
                for sample in real_data
            }

            # Process completed tasks with progress bar
            for future in tqdm(as_completed(futures), total=len(futures), desc="Paraphrasing"):
                try:
                    variations = future.result()
                    if variations:
                        paraphrase_results.extend(variations)
                except Exception as e:
                    sample = futures[future]
                    print(f"\nâš  Error processing sample: {e}")

        all_synthetic.extend(paraphrase_results)
        print(f"  âœ“ Generated {len(paraphrase_results)} paraphrased samples")

        # Save Strategy 1 checkpoint
        self._save_checkpoint(paraphrase_results, "checkpoint_1_paraphrase.json")

        # Strategy 2: Style Transfer (10% of samples) - PARALLEL
        print("\nðŸŽ¨ Strategy 2: Style Transfer (Parallel)...")
        print(f"  Using {self.config.max_workers} concurrent workers")
        style_transfer_count = len(real_data) // 10
        style_samples = random.sample(real_data, min(style_transfer_count, len(real_data)))

        style_results = []
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # Submit all tasks
            futures = {
                executor.submit(self._process_style_transfer_sample, sample): sample
                for sample in style_samples
            }

            # Process completed tasks with progress bar
            for future in tqdm(as_completed(futures), total=len(futures), desc="Style Transfer"):
                try:
                    result = future.result()
                    if result:
                        style_results.append(result)
                except Exception as e:
                    sample = futures[future]
                    print(f"\nâš  Error processing sample: {e}")

        all_synthetic.extend(style_results)
        print(f"  âœ“ Generated {len(style_results)} style-transferred samples")

        # Save Strategy 2 checkpoint
        self._save_checkpoint(all_synthetic, "checkpoint_2_after_style_transfer.json")

        # Strategy 3: Zero-shot Generation (fill remaining) - PARALLEL
        current_count = len(all_synthetic)
        remaining = self.config.target_size - current_count

        if remaining > 0:
            print(f"\nâœ¨ Strategy 3: Zero-shot Generation (Parallel)...")
            print(f"  Generating {remaining} new samples")
            print(f"  Using {self.config.max_workers} concurrent workers")

            # Create tasks: each generates 5 samples
            num_tasks = (remaining + 4) // 5
            tasks = []
            for _ in range(num_tasks):
                # Alternate between AI and HUMAN
                label = "AI" if random.random() < 0.5 else "HUMAN"
                tasks.append((label, 5))

            zero_shot_results = []
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                # Submit all tasks
                futures = {
                    executor.submit(self.generate_zero_shot, label, num_samples): (label, num_samples)
                    for label, num_samples in tasks
                }

                # Process completed tasks with progress bar
                for future in tqdm(as_completed(futures), total=len(futures), desc="Zero-shot"):
                    try:
                        samples = future.result()
                        if samples:
                            zero_shot_results.extend(samples)
                    except Exception as e:
                        print(f"\nâš  Error generating zero-shot: {e}")

            all_synthetic.extend(zero_shot_results)
            print(f"  âœ“ Generated {len(zero_shot_results)} zero-shot samples")

            # Save Strategy 3 checkpoint
            self._save_checkpoint(all_synthetic, "checkpoint_3_after_zero_shot.json")

        # Trim to target size
        all_synthetic = all_synthetic[:self.config.target_size]

        print(f"\nâœ… Total synthetic samples: {len(all_synthetic)}")

        # Save final checkpoint before trimming
        self._save_checkpoint(all_synthetic, "checkpoint_final_trimmed.json")

        return all_synthetic

    def save_synthetic_data(self, synthetic_data: List[Dict]):
        """Save synthetic data to output directory"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save all synthetic data
        output_path = output_dir / "all_synthetic.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(synthetic_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ Saved synthetic data to {output_path}")

        # Save by generation method
        methods = {}
        for sample in synthetic_data:
            method = sample.get("source", "unknown")
            if method not in methods:
                methods[method] = []
            methods[method].append(sample)

        for method, samples in methods.items():
            method_path = output_dir / f"{method}.json"
            with open(method_path, "w", encoding="utf-8") as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
            print(f"  âœ“ Saved {len(samples)} samples to {method_path.name}")

        # Statistics
        print(f"\nðŸ“Š Synthetic Data Statistics:")
        print(f"  Total: {len(synthetic_data):,}")
        ai_count = sum(1 for s in synthetic_data if s["label"] == "AI")
        human_count = len(synthetic_data) - ai_count
        print(f"  AI: {ai_count:,} ({ai_count/len(synthetic_data)*100:.1f}%)")
        print(f"  HUMAN: {human_count:,} ({human_count/len(synthetic_data)*100:.1f}%)")


def main():
    # Define default output path as absolute path
    DEFAULT_OUTPUT_DIR = str(PROJECT_ROOT / "data" / "synthetic")

    parser = argparse.ArgumentParser(
        description="Generate synthetic training data using Gemini Flash"
    )
    parser.add_argument(
        "--real_data",
        type=str,
        default="/Users/taesooa/Desktop/Swift/Naver-Blog-AI-Detector/Naver-Blog-AI-Detector/Naver-Blog-AI-Detector-ML/Data-Preprocessing/data/processed/training_data.json",
        help="Path to real training data"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=DEFAULT_OUTPUT_DIR,
        help="Output directory for synthetic data"
    )
    parser.add_argument(
        "--target_size",
        type=int,
        default=None,
        help="Target number of synthetic samples (default: 3x real data size)"
    )
    parser.add_argument(
        "--multiplier",
        type=float,
        default=3.0,
        help="Multiplier for real data size (default: 3.0)"
    )
    parser.add_argument(
        "--variations",
        type=int,
        default=5,
        help="Number of variations per sample"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.9,
        help="Gemini generation temperature"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gemini-flash-latest",
        help="Gemini model to use"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=20,
        help="Number of concurrent workers for parallel API calls (default: 20)"
    )

    args = parser.parse_args()

    # Load real data to calculate target size
    import json
    with open(args.real_data, 'r', encoding='utf-8') as f:
        real_data_temp = json.load(f)

    real_data_count = len(real_data_temp)

    # Calculate target size: 3x real data size if not specified
    if args.target_size is None:
        target_size = int(real_data_count * args.multiplier)
        print(f"\nðŸ“Š Auto-calculating target size:")
        print(f"  Real data: {real_data_count:,} samples")
        print(f"  Multiplier: {args.multiplier}x")
        print(f"  Target synthetic: {target_size:,} samples")
    else:
        target_size = args.target_size
        print(f"\nðŸ“Š Using specified target size: {target_size:,} samples")

    # Create config
    config = SyntheticConfig(
        real_data_path=args.real_data,
        output_dir=args.output,
        target_size=target_size,
        gemini_model=args.model,
        variations_per_sample=args.variations,
        temperature=args.temperature,
        max_workers=args.workers,
    )

    print("\n" + "=" * 70)
    print("ðŸ¤– Gemini Flash Synthetic Data Generator (Parallel)")
    print("=" * 70)
    print(f"\nConfiguration:")
    print(f"  Real Data: {config.real_data_path}")
    print(f"  Real Data Count: {real_data_count:,}")
    print(f"  Output: {config.output_dir}")
    print(f"  Target Size: {config.target_size:,} ({args.multiplier}x real data)")
    print(f"  Variations/Sample: {config.variations_per_sample}")
    print(f"  Model: {config.gemini_model}")
    print(f"  Temperature: {config.temperature}")
    print(f"  Parallel Workers: {config.max_workers}")

    # Generate synthetic data
    generator = GeminiSyntheticGenerator(config)
    synthetic_data = generator.generate_all_synthetic_data()

    # Save results
    generator.save_synthetic_data(synthetic_data)

    print("\nâœ… Synthetic data generation complete!")


if __name__ == "__main__":
    main()
